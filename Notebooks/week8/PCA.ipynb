{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example of Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[10,20,10],\n",
    "              [2,5,2],\n",
    "              [8,17,7],\n",
    "              [9,20,10],\n",
    "              [12,22,11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 20 10]\n",
      " [ 2  5  2]\n",
      " [ 8 17  7]\n",
      " [ 9 20 10]\n",
      " [12 22 11]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first try to drive the principle components directly without using scikit-learn Decomposition module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.2  25.3  13.5 ]\n",
      " [25.3  46.7  24.75]\n",
      " [13.5  24.75 13.5 ]]\n"
     ]
    }
   ],
   "source": [
    "# To dod PCA, you need to center your matrix\n",
    "X = np.mat(X) \n",
    "meanVals = X.mean(axis=0)\n",
    "A = X - meanVals           # A is the zero-mean (centered) version of X\n",
    "C = np.cov(A, rowvar=0)    # C is the covarianvce matrix of X\n",
    "print(C) # ypu had 3 features so you end up with a 3 * 3 convariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Every column is a a feature so you need the mean of the featureand you need to subtract that from the original matrix and after that you compute the convariance </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 8.2, 16.8,  8. ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanVals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The first mean is 8.2. Second mean is 16.8 and third mean is 8.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.2  25.3  13.5 ]\n",
      " [25.3  46.7  24.75]\n",
      " [13.5  24.75 13.5 ]]\n"
     ]
    }
   ],
   "source": [
    "# Note that C = (1/(N-1)) A.T*A\n",
    "# you can compute C and verify that it is the same exact thing that numpy is doing is in cell (4)\n",
    "N = np.shape(X)[0]\n",
    "print(np.dot(A.T,A)/(N-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can obtain eigenvalues and eigenvectors of the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen Values: [73.71803604  0.38355337  0.29841058] \n",
      "\n",
      "Eigen Vectors:\n",
      " [[ 0.43405692  0.89979879 -0.04423488]\n",
      " [ 0.79486757 -0.40562416 -0.45128105]\n",
      " [ 0.42400487 -0.16072079  0.89128486]]\n"
     ]
    }
   ],
   "source": [
    "# with numpy we can obtain the eigen vectors of C, you need to have computed C which we supply as a parameter \n",
    "# and get eigen values and vectors at once\n",
    "eigen_values, eigen_vectors = np.linalg.eig(C)\n",
    "print(\"Eigen Values:\", eigen_values, \"\\n\")\n",
    "print(\"Eigen Vectors:\\n\", eigen_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We have eigen values: we haveM a very large first component with 73.718, and almost nothing on the second and third components. </p>\n",
    "    <p>we know that [ 0.43405692  0.89979879 -0.04423488] is the most important eigen vector of my data set. <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No we can transform the full data into the new feature space based on the eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.17288843e+00  1.98923940e-04  2.58847587e-01]\n",
      " [-1.46146195e+01  1.71937350e-01  2.51663442e-01]\n",
      " [-3.51842744e-01 -1.00363798e-01 -9.72694089e-01]\n",
      " [ 3.73883152e+00 -8.99599868e-01  3.03082462e-01]\n",
      " [ 7.05474229e+00  8.27827393e-01  1.59100599e-01]]\n"
     ]
    }
   ],
   "source": [
    "# we transform the old features by using the transpose of the eigen vectors \n",
    "# this is similar to the slide of U  *A \n",
    "\n",
    "newFeatures = eigen_vectors.T\n",
    "XTrans = np.dot(newFeatures, A.T)\n",
    "print(XTrans.T) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### this is the transformed matrix in the new space with all the dimensions for our data points but we are not going to need all of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>My new features, we just need only one feature in the new space./p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, typically, we want a lower-dimensional space. We can sort the eigenvectors in the decreasing order of their eigenvalues and take the top k. In the example below, we'll take only the top first principal component (since it has the largest eigenvalue, no sorting necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.17288843]\n",
      " [-14.6146195 ]\n",
      " [ -0.35184274]\n",
      " [  3.73883152]\n",
      " [  7.05474229]]\n"
     ]
    }
   ],
   "source": [
    "# the point is that we can drop a feature and take only one of the eigen vectors, \n",
    "# take only this [ 0.43405692  0.89979879 -0.04423488] produced in cell 7 which is everything for the first dimension and we use \n",
    "# that in the dot product  with A.T (transpose of A) \n",
    "# and gives you only one dimension in the new space.\n",
    "reducedFeatures = eigen_vectors[:,0].T\n",
    "reducedXTrans = np.dot(reducedFeatures, A.T)\n",
    "print(reducedXTrans.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the new space, this is the only feature that we have.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also use Scikit-learn decomposition module to do the same thing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In Scitit-learn, we don't ever look at this calculations.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rejalu1\\.conda\\envs\\cmdpy37\\lib\\site-packages\\sklearn\\utils\\validation.py:590: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "pca = decomposition.PCA(svd_solver='randomized')\n",
    "XTrans = pca.fit_transform(X) # fit your data into PCA and you immediately get your transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.173 -0.    -0.259]\n",
      " [14.615 -0.172 -0.252]\n",
      " [ 0.352  0.1    0.973]\n",
      " [-3.739  0.9   -0.303]\n",
      " [-7.055 -0.828 -0.159]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(XTrans) # this is your transform, the one that we got through multiple steps in cell 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This is just computing your new features</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The remaining part of this notebook, is another example of using PCA for dimensionality reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a new matrix of data\n",
    "M = np.array([[2.5, 2.4],\n",
    "           [0.5, 0.7],\n",
    "           [2.2, 2.9],\n",
    "           [1.9, 2.2],\n",
    "           [3.1, 3.0],\n",
    "           [2.3, 2.7],\n",
    "           [2, 1.6],\n",
    "           [1, 1.1],\n",
    "           [1.5, 1.6],\n",
    "           [1.1, 0.9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Mean Matrix:\n",
      " [[ 0.69  0.49]\n",
      " [-1.31 -1.21]\n",
      " [ 0.39  0.99]\n",
      " [ 0.09  0.29]\n",
      " [ 1.29  1.09]\n",
      " [ 0.49  0.79]\n",
      " [ 0.19 -0.31]\n",
      " [-0.81 -0.81]\n",
      " [-0.31 -0.31]\n",
      " [-0.71 -1.01]] \n",
      "\n",
      "Covariance Matrix:\n",
      " [[0.617 0.615]\n",
      " [0.615 0.717]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "meanM = M.mean(axis=0) # compute the mean \n",
    "# center the matrix (matrix - mean)\n",
    "MC = M - meanM                 # MC is the zero-mean (centered) version of M\n",
    "\n",
    "# you get the convariance\n",
    "CovM = np.cov(MC, rowvar=0)    # CovM is the covarianvce matrix of M\n",
    "print(\"Zero Mean Matrix:\\n\", MC,\"\\n\")\n",
    "print(\"Covariance Matrix:\\n\", CovM,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      " [0.049 1.284] \n",
      "\n",
      "Eigenvectors:\n",
      " [[-0.735 -0.678]\n",
      " [ 0.678 -0.735]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "eigVals, eigVecs = np.linalg.eig(CovM)\n",
    "print(\"Eigenvalues:\\n\", eigVals,\"\\n\")\n",
    "print(\"Eigenvectors:\\n\", eigVecs,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You have your eigen values (where 1.284 is the highest) and eigen vectors of our only two features and this one,  [ 0.678 -0.735], is more important.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.678 -0.735]\n"
     ]
    }
   ],
   "source": [
    "# We are going to just take one of these vectors\n",
    "# So this value (1.284 in cell 15) was higher and this is why we are taking this vector  [ 0.678 -0.735] for your transformation\n",
    "newFeatures = eigVecs[:,1].T \n",
    "print(newFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.828]\n",
      " [ 1.778]\n",
      " [-0.992]\n",
      " [-0.274]\n",
      " [-1.676]\n",
      " [-0.913]\n",
      " [ 0.099]\n",
      " [ 1.145]\n",
      " [ 0.438]\n",
      " [ 1.224]]\n"
     ]
    }
   ],
   "source": [
    "MTrans = np.dot(newFeatures, MC.T)\n",
    "print(np.mat(MTrans).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>So instead of two features, you are working with just one feature.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.828]\n",
      " [ 1.778]\n",
      " [-0.992]\n",
      " [-0.274]\n",
      " [-1.676]\n",
      " [-0.913]\n",
      " [ 0.099]\n",
      " [ 1.145]\n",
      " [ 0.438]\n",
      " [ 1.224]]\n"
     ]
    }
   ],
   "source": [
    "# Instead we can use scikit-learn's decomposition.PCA and specifiy the number of components\n",
    "\n",
    "pca2 = decomposition.PCA(n_components=1)\n",
    "MTrans2 = pca2.fit_transform(M)\n",
    "print(MTrans2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In Scikit-learn  you would tell it how many components you want. You can print them all but yo can tell it how many components you have and get just the same exact answer. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
